{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from load_and_preprocess_data import TrainDataInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile training_utils.py\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input as xcept_preproc\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from load_and_preprocess_data import TrainDataInfo\n",
    "from image_utilities import ImageUtilities\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# This is a light wrapper that holds the TrainDataInfo object, and has some basic utils\n",
    "class TrainingUtils:\n",
    "    def __init__(self, all_train_info, img_size_3d, batch_size):\n",
    "        self.train_info = all_train_info\n",
    "        self.num_countries = len(all_train_info.country_names)\n",
    "        csizes =  [len(all_train_info.filename_map[c]) for c in all_train_info.country_names]\n",
    "        self.total_file_count = sum(csizes)\n",
    "        self.img_size_3d = img_size_3d\n",
    "        self.img_size_2d = img_size_3d[:2]\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        \n",
    "    def indices_to_labels(self, y_idxs):\n",
    "        return [self.train_info.country_names[y] for y in y_idxs]\n",
    "    \n",
    "    \n",
    "    def indices_to_labels_disp(self, y_idxs):\n",
    "        return [self.train_info.country_names_disp[y] for y in y_idxs]\n",
    "    \n",
    "    \n",
    "    def print_classification_report(self, y_true, y_pred):\n",
    "        y_true_lbl = self.indices_to_labels(y_true)\n",
    "        y_pred_lbl = self.indices_to_labels(y_pred)\n",
    "        print(sklearn.metrics.classification_report(y_true_lbl, y_pred_lbl))\n",
    "        \n",
    "\n",
    "    def display_heatmaps(self, y_true, y_pred):\n",
    "        y_true_lbl = self.indices_to_labels_disp(y_true)\n",
    "        y_pred_lbl = self.indices_to_labels_disp(y_pred)\n",
    "        fig,axs = plt.subplots(1,2, figsize=(16,6))\n",
    "        cm = sklearn.metrics.confusion_matrix(y_true_lbl, y_pred_lbl)\n",
    "        cm_norm = (cm.T / np.sum(cm, axis=1)).T\n",
    "        sns.heatmap(cm, annot=True, ax=axs[0], linewidths=0.5, cbar=False)\n",
    "        sns.heatmap(cm_norm, annot=True, ax=axs[1], linewidths=0.5, cbar=False)\n",
    "        for i in range(2):\n",
    "            axs[i].set_xticks(np.arange(self.num_countries)+0.5)\n",
    "            axs[i].set_xticklabels(self.train_info.country_names_disp)\n",
    "            axs[i].set_yticks(np.arange(self.num_countries)+0.5)\n",
    "            axs[i].set_yticklabels(self.train_info.country_names_disp)\n",
    "            axs[i].xaxis.set_ticks_position('top')\n",
    "            plt.setp(axs[i].get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\n",
    "            plt.setp(axs[i].get_yticklabels(), rotation=0, va=\"top\") \n",
    "        axs[0].set_title('Confusion Matrix (Unscaled)')\n",
    "        axs[1].set_title('Confusion Matrix (Normalized)')\n",
    "        \n",
    "        \n",
    "    def display_training_hist(self, hist):\n",
    "        fig, axs = plt.subplots(1,2, figsize=(12,6))\n",
    "        axs[0].plot(hist.history['loss'], label='Training Loss')\n",
    "        axs[0].plot(hist.history['val_loss'], label='Testing Loss')\n",
    "        axs[0].set_ylim(bottom=0)\n",
    "        axs[0].legend()\n",
    "        axs[1].plot(hist.history['accuracy'], label='Training accuracy')\n",
    "        axs[1].plot(hist.history['val_accuracy'], label='Testing accuracy')\n",
    "        axs[1].set_ylim([-0.05, 1.05])\n",
    "        axs[1].legend()\n",
    "        axs[0].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        \n",
    "        \n",
    "    def train_data_generator_disk(self,\n",
    "        train_split_index,\n",
    "        img_size_2d,\n",
    "        use_sample_weighting=False,\n",
    "        label_smoothing_value=0.05,\n",
    "        preproc_func=lambda x: x):\n",
    "\n",
    "        \"\"\"\n",
    "        The train_info object has indexed all the filenames for all available data. \n",
    "        It has precomputed sampling probabilities, sample weights, and train-test splits\n",
    "        Supply label_smoothing_value=0 to disable it\n",
    "        For preproc_func we'll supply the inception_v3 preprocess_inputs function\n",
    "        \"\"\"\n",
    "\n",
    "        def out_generator():\n",
    "            file_map = self.train_info.traintest_splits[train_split_index]['train']\n",
    "            total_ct = sum([len(file_map[c]) for c in file_map.keys()])\n",
    "\n",
    "            # Consider the total image count 1 epoch, even though not all images will be sampled\n",
    "            for i in range(total_ct):\n",
    "                img_path, country_idx = self.train_info.sample_filename('train', train_split_index)\n",
    "                sample_wgt = self.train_info.weight_scalars[country_idx]\n",
    "\n",
    "                img = ImageUtilities.load_image(img_path, preproc_func)\n",
    "                img = ImageUtilities.augment_image(img, resize_to=img_size_2d)\n",
    "\n",
    "                ans = np.zeros(shape=(self.num_countries,), dtype='float32')\n",
    "                ans = ans + label_smoothing_value\n",
    "                ans[country_idx] = 1.0 - label_smoothing_value\n",
    "\n",
    "                if use_sample_weighting:\n",
    "                    yield (img, ans, sample_wgt)\n",
    "                else:\n",
    "                    yield (img, ans)\n",
    "\n",
    "        return out_generator\n",
    "\n",
    "\n",
    "    def train_data_generator_memory(self,\n",
    "        train_split_index,\n",
    "        img_size_2d,\n",
    "        use_sample_weighting=False,\n",
    "        label_smoothing_value=0.05,\n",
    "        preproc_func=lambda x: x):\n",
    "    \n",
    "        \"\"\"\n",
    "        This method is same as above, but loads all training images into memory (before\n",
    "        augmentation).  This is pretty heavy for most systems, thus the previous version\n",
    "        that loads from disk on-the-fly is available and tested.\n",
    "\n",
    "        The train_info object has indexed all the filenames for all available data. \n",
    "        It has precomputed sampling probabilities, sample weights, and train-test splits\n",
    "        Supply label_smoothing_value=0 to disable it\n",
    "        For preproc_func we'll supply the inception_v3 preprocess_inputs function\n",
    "        \"\"\"\n",
    "\n",
    "        file_map = self.train_info.traintest_splits[train_split_index]['train']\n",
    "        print(f'Filling image cache')\n",
    "        image_cache = {}\n",
    "        for country in self.train_info.country_names:\n",
    "            for f in file_map[country]:\n",
    "                image_cache[f] = ImageUtilities.load_image(f, preproc_func)\n",
    "\n",
    "        def out_generator():\n",
    "            total_ct = len(image_cache)\n",
    "            # Consider the total image count 1 epoch, even though not all images will be sampled\n",
    "            for i in range(total_ct):\n",
    "                img_path, country_idx = self.train_info.sample_filename('train', train_split_index)\n",
    "                sample_wgt = self.train_info.weight_scalars[country_idx]\n",
    "\n",
    "                img = image_cache[img_path]\n",
    "                img = ImageUtilities.augment_image(img, resize_to=img_size_2d)\n",
    "\n",
    "                ans = np.zeros(shape=(self.num_countries,), dtype='float32')\n",
    "                ans = ans + label_smoothing_value\n",
    "                ans[country_idx] = 1.0 - label_smoothing_value\n",
    "\n",
    "                if use_sample_weighting:\n",
    "                    yield (img, ans, sample_wgt)\n",
    "                else:\n",
    "                    yield (img, ans)\n",
    "\n",
    "        return out_generator\n",
    "\n",
    "\n",
    "    def generator_to_dataset(self,\n",
    "        gener_obj,\n",
    "        img_size_3d,\n",
    "        n_classes,\n",
    "        batch_size=32,\n",
    "        with_sample_weighting=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Take a generator produced above and wrap it in a tf.data.Dataset.  \n",
    "        from_generator requires and image size, hence why it is passed in again.\n",
    "        \"\"\"\n",
    "        if with_sample_weighting:\n",
    "            out_dataset = tf.data.Dataset.from_generator( \n",
    "                gener_obj,\n",
    "                (tf.float32, tf.float32, tf.float32), \n",
    "                (tf.TensorShape(img_size_3d), tf.TensorShape((n_classes,)), tf.TensorShape([])))\n",
    "        else:\n",
    "            out_dataset = tf.data.Dataset.from_generator( \n",
    "                gener_obj,\n",
    "                (tf.float32, tf.float32), \n",
    "                (tf.TensorShape(img_size_3d), tf.TensorShape((n_classes,))))\n",
    "\n",
    "        if batch_size:\n",
    "            out_dataset = out_dataset.batch(batch_size)\n",
    "\n",
    "        return out_dataset\n",
    "\n",
    "\n",
    "    def create_test_dataset(self,\n",
    "        test_split_index,\n",
    "        img_size_3d,\n",
    "        preproc_func,\n",
    "        batch_size=1):\n",
    "        \n",
    "        \"\"\"\n",
    "        This just iterates through all images, in order, on the test side of the split.\n",
    "        No augmentation is applied to the test dataset\n",
    "        Default batch_size=1 because it avoids OOM problems, and it's not bad for 145 test images\n",
    "        \"\"\"\n",
    "\n",
    "        def gen_func():\n",
    "            file_map = self.train_info.traintest_splits[test_split_index]['test']\n",
    "            for c_idx in range(self.num_countries):\n",
    "                country = self.train_info.country_names[c_idx]\n",
    "                for img_path in file_map[country]:\n",
    "                    img = ImageUtilities.load_image(img_path, preproc_func)\n",
    "                    img = cv2.resize(img, img_size_3d[:2])\n",
    "                    ans = np.zeros(shape=(self.num_countries,), dtype='float32')\n",
    "                    ans[c_idx] = 1.0\n",
    "                    yield (img, ans)\n",
    "\n",
    "        return self.generator_to_dataset(gen_func, img_size_3d, self.num_countries, batch_size, with_sample_weighting=False)\n",
    "\n",
    "    \n",
    "    # Iterate over all cross-val splits\n",
    "    def train_one_fold(self, model_gen_func, kfold_index, epochs=20, optimizer=None, model=None, model_prefix=None):\n",
    "        \"\"\"\n",
    "        Can pass in a pre\n",
    "        \"\"\"\n",
    "        #gen = self.train_data_generator_disk(\n",
    "        gen = self.train_data_generator_memory(\n",
    "            kfold_index,\n",
    "            self.img_size_2d,\n",
    "            preproc_func=xcept_preproc,\n",
    "            use_sample_weighting=True)\n",
    "\n",
    "        train_ds = self.generator_to_dataset(gen, self.img_size_3d, self.num_countries, self.batch_size, with_sample_weighting=True)\n",
    "        test_ds = self.create_test_dataset(kfold_index, self.img_size_3d, xcept_preproc)\n",
    "\n",
    "        # Default RMSprop optimizer with initial LR=0.001\n",
    "        if optimizer is None:\n",
    "            optimizer = keras.optimizers.RMSprop(3e-4)\n",
    "\n",
    "        if model is None:\n",
    "            print(f'Creating new model for fold {kfold_index}')\n",
    "            model = model_gen_func()\n",
    "            model.compile(optimizer=optimizer,\n",
    "                          loss='categorical_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "            \n",
    "        \n",
    "        model_prefix = 'weights'\n",
    "        if model_prefix:\n",
    "             model_prefix = model_prefix.rstrip('_')\n",
    "                \n",
    "        model_save_file = f'{model_prefix}_kfold_{kfold_index}.hdf5'\n",
    "        \n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='min')\n",
    "        mcp_save = ModelCheckpoint(model_save_file, save_best_only=True, monitor='val_loss', mode='min')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.25, patience=3, verbose=1, mode='min')\n",
    "\n",
    "        training_history = model.fit(\n",
    "            x=train_ds,\n",
    "            epochs=epochs,\n",
    "            validation_data=test_ds,\n",
    "            callbacks=[mcp_save, reduce_lr])\n",
    "\n",
    "        model.load_weights(filepath = model_save_file)\n",
    "        true_labels = []\n",
    "        pred_labels = []\n",
    "        for batch in test_ds:\n",
    "            img_batch, label_batch = batch \n",
    "            calc_labels = model(img_batch).numpy()\n",
    "            true_labels.append(np.argmax(label_batch, axis=1))\n",
    "            pred_labels.append(np.argmax(calc_labels, axis=1))\n",
    "\n",
    "        true_labels = np.concatenate(true_labels, axis=0)\n",
    "        pred_labels = np.concatenate(pred_labels, axis=0)\n",
    "        print(f'Cross-val fold {kfold_index} confusion matrix:')\n",
    "        self.print_classification_report(true_labels, pred_labels)\n",
    "\n",
    "        return model, true_labels, pred_labels, training_history\n",
    "    \n",
    "    \n",
    "    def train_eval_kfold_crossval(self, model_gen_func, epochs=20, model_prefix=None):\n",
    "        \"\"\"\n",
    "        Input is a model that takes a batch of [_, 299, 299, 3] images & produces [_, 11] outputs\n",
    "        \"\"\"\n",
    "        true_labels = []\n",
    "        pred_labels = []\n",
    "        hist = None\n",
    "        for kfold_index in range(5):\n",
    "            model, fold_true, fold_pred, hist = self.train_one_fold(model_gen_func, kfold_index, epochs, model_prefix)\n",
    "\n",
    "            #print(f'Cross-val fold {kfold_index} confusion matrix:')\n",
    "            #print(sklearn.metrics.confusion_matrix(fold_true, fold_pred))\n",
    "            true_labels.extend(fold_true)\n",
    "            pred_labels.extend(fold_pred)\n",
    "\n",
    "        # Returns last model and training_history, all true/pred labels for all images\n",
    "        return model, true_labels, pred_labels, hist\n",
    "    \n",
    "    def extract_features_to_disk(self, fex_model, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_utils import TrainingUtils\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
